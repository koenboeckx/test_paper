\chapter{Conclusion}
\label{ch:conclusion}
As mentioned in the introduction, the goal of this thesis is twofold:
\begin{enumerate}
    \item To develop an algorithmic model of a battlefield with opposing agents and a terrain that can both prohibit movement and visibility.
    \item To assess the feasibility of using state-of-the-art RL algorithms on this model to develop strategies.
\end{enumerate}
The first goal has been met: an algorithmic model has been designed and implemented that tries to take into account the most important elements of real-life combat situations. Furthermore, the implementation has been done in such a way that the model can be easily extended. These extensions may include things like additional state variables (e.g. fuel consumption), the addition of new restrictions on the behavior of agents, the addition of extra actions that an agent may take, creating obstacles that inhibit movement but not visibility or vice versa, different kinds of agents, \ldots\\
As for the second goal, a number of reinforcement algorithms have been implemented. These include algorithms from the policy gradient family that directly try to push the agents policy in the right direction, like REINFORCE or Actor-Critic algorithms, and algorithms from the Q-learning family where the goal is to make an accurate estimation of the Q-value and derive a policy from that. Examples of the latter family are Independent Q-learning and QMix.\\
The results of these algorithms is mixed. Purely based on performance, the simplest of these algorithms, REINFORCE, performs best. This might be because it's the best algorithm, because it is less sensitive to hyperparameters and the optimal set of hyperparameters for the other algorithms has not been found, or due to implementation issues with the other algorithms.\\
QMix is a true multi-agent algorithm, developed to stimulate coordination between agents and by doing so develop coordinated strategies. However, the limited experimentation performed with QMix on the environment model has not definitely shown this coordination. Furthermore, QMix has convergence problems when the board size becomes larger.\\
Care was taken during implementation to keep both the neural network and the observations for the different agents as generic as possible. This made it possible to easily transfer networks between agents, even of different teams. This in turn allowed the use of model transfer to learn to play against teams of increasing strength. This procedure works well and produces good results. However, more evaluation of the developed strategies is needed.\\
Personally, I would like to say that implementing these algorithms from scratch is not always an easy or straightforward feat. Some papers don't always make explicit which technical assumptions have been made, thus during implementation certain well considered choices had to be made in the hope that they align with the intention of the authors. Another difficulty is the large number of hyperparameters that each algorithm has. As mentioned previously, choosing a good combination of hyperparameters can be hard, and certain algorithms are very sensitive to a good choice of these parameters.\\
This work provided several contributions to the Belgian Defence:
\begin{itemize}
    \item Participation in the IRIS project mentioned in the introduction.
    \item Development of a certain expertise in a domain that becomes more and more important, namely the deployment of AI in military operations.
    \item The start of the development of a tool that has the potential to contribute significantly to the decision making process.
\end{itemize}
Experience has shown that developing and maintaining AI-systems like the one proposed in this thesis requires a lot of resources. This not only includes computing power and data, but also personnel and time. The need for computing power is high because training these kind of systems is computationally expensive. Supervised and unsupervised learning systems require lots of training data before they become operational.
Some aspects of AI have a steep learning curve, thus development needs engineers and programmers that have the required background. At least as important is the ability to interface with the end-users and the domain specialists to develop something that corresponds to their needs. Development of an AI-system is time-consuming because it will go through several iterations of back-and-forth between the development team and the domain specialists.\\
To conclude, I would like to say that using state-of-the-art RL algorithms to develop battlefield strategies seems to be possible; however it will take a lot of time and effort to develop something that is both useful \& usable. An additional difficulty would be to deploy this in a vehicle where resources are limited and that can process all this information in real time.