%Bijlagen eindwerk
%\thispagestyle{fancy}
%\chapter*{Bijlagen}
%\thispagestyle{plain}
\begin{appendices}
%\appendix

\chapter{Independent Q-Learning}
\label{app:iql}

\begin{minted}{python}
def train(env, agents, lr=0.0001):
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    with SummaryWriter() as writer:
        
        # create and initialize model for agent
        input_shape = (1, env.board_size, env.board_size)
        for agent in agents:
            agent.set_model(input_shape, env.n_actions, lr, device)

        get_epsilon = create_temp_schedule(1.0, 0.1, 500000)

        reward_sum = 0 # keep track of average reward
        n_terminated = 0

        buffers = [ReplayBuffer(buffer_size) for _ in agents]
        state = env.get_init_game_state()
        
        for step_idx in range(int(n_steps)):
            
            eps = get_epsilon(step_idx)
            actions = [0, 0, 0, 0]
            for agent in env.agents:
                if agent in agents:
                    # with prob epsilon, select random action a;
                    # otherwise a = argmax Q(s, .)
                    actions[agent.idx] = agent.get_action(state, 
                                            epsilon=eps, device=device)
                else:
                    # for other agents => pick random action
                    actions[agent.idx] = agent.get_action(state)
            
            # Execute actions, get next state and rewards
            next_state = env.step(state, actions)
            reward = env.get_reward(next_state)

            done = False
            if env.terminal(next_state) != 0:
                n_terminated += 1
                reward_sum += reward[0]
                done = True
                next_state =  env.get_init_game_state()

            # Store transition (s, a, r, s') in replay buffer
            for idx, agent in enumerate(agents):
                exp = Experience(state=state, action=actions[agent.idx],
                                 reward=reward[agent.idx],
                                 next_state=next_state, done=done)
                buffers[idx].insert(exp)

            if len(buffers[0]) >= replay_start_size:
                for agent_idx, agent in enumerate(agents):
                    agent.model.optim.zero_grad()
                    # Sample minibatch and compute loss
                    minibatch = buffers[agent_idx].sample(mini_batch_size)
                    loss = calc_loss(agent, minibatch, gamma, device=device)

                    writer.add_scalar('agent{}_loss'.format(agent.idx),
                                        loss.item(), step_idx)
                
                    # perform training step 
                    loss.backward()
                    agent.model.optim.step()

                    if step_idx > 0 and step_idx % sync_rate == 0:
                        print('iteration {} - syncing ...'.format(step_idx))
                        agent.sync_models()

            state = next_state


\end{minted}

\end{appendices}


