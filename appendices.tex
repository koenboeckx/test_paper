\begin{appendices}
%\appendix
These appendices show how the major algorithms in this thesis (IQL, IAC, QMix) have been implemented. All imports, loggings and technical details that are less relevant have been removed for clarity. This means that as such, these algorithms are not functional.\\ To experiment with the algorithms, please consult the {\tt GitHub} reference page:\\ \url{https://github.com/koenboeckx/VKHO}

\chapter{Independent Q-Learning}
\label{app:iql}

\begin{minted}{python}
def train(env, agents, lr=0.0001):
    
    # create and initialize model for agent
    input_shape = (1, env.board_size, env.board_size)
    for agent in agents:
        agent.set_model(input_shape, env.n_actions, lr)

    get_epsilon = create_temp_schedule(1.0, 0.1, 500000)

    reward_sum = 0 # keep track of average reward
    n_terminated = 0

    buffers = [ReplayBuffer(buffer_size) for _ in agents]
    state = env.get_init_game_state()
    
    for step_idx in range(int(n_steps)):
        
        eps = get_epsilon(step_idx)
        actions = [0, 0, 0, 0]
        for agent in env.agents:
            if agent in agents:
                # with prob epsilon, select random action a;
                # otherwise a = argmax Q(s, .)
                actions[agent.idx] = agent.get_action(state, epsilon=eps)
            else:
                # for other agents => pick random action
                actions[agent.idx] = agent.get_action(state)
        
        # Execute actions, get next state and rewards
        next_state = env.step(state, actions)
        reward = env.get_reward(next_state)

        done = False
        if env.terminal(next_state) != 0:
            n_terminated += 1
            reward_sum += reward[0]
            done = True
            next_state =  env.get_init_game_state()

        # Store transition (s, a, r, s') in replay buffer
        for idx, agent in enumerate(agents):
            exp = Experience(state=state, action=actions[agent.idx],
                             reward=reward[agent.idx],
                             next_state=next_state, done=done)
            buffers[idx].insert(exp)

        if len(buffers[0]) >= replay_start_size:
            for agent_idx, agent in enumerate(agents):
                agent.model.optim.zero_grad()
                # Sample minibatch and compute loss
                minibatch = buffers[agent_idx].sample(mini_batch_size)
                loss = calc_loss(agent, minibatch, gamma, device=device)

                # perform training step 
                loss.backward()
                agent.model.optim.step()

                if step_idx > 0 and step_idx % sync_rate == 0:
                    agent.sync_models()

        state = next_state


\end{minted}

\chapter{Independent Actor-Critic}
\label{app:iac}
\begin{minted}{python}

class IACAgent(Agent):
    "An Actor-Critic Agent"
    def act(self, obs):
        "Select an action based on an observation"

        unavailable_actions = self.env.get_unavailable_actions()[self]
        # compute the logits based on network policy model
        _, logits = self.model([obs])
        
        # for unavailable actions, set logits to very low value
        for action in unavailable_actions:
            logits[action.id] = -np.infty
        
        # create a prob distribution based on logits and sample from it
        action_idx = Categorical(logits=logits).sample().item()
        return self.actions[action_idx]

    def update(self, batch):
        "Perform one update step according to the IAC algorithm"
        _, actions, rewards, _, dones, observations, hidden,\ 
            next_obs, unavail = zip(*batch)
        
        # only perform updates on actions performed while alive
        self_alive = [idx for idx, obs in enumerate(observations) if obs[self].alive]

        rewards = torch.tensor([reward[self.team] for reward in rewards])
        
        # use the actor-critic network to compute V(s) and logits
        values, logits, h = self.model(observations, hidden) 
        next_vals, _, _   = self.target(next_obs, h)
        
        # pick the actions that were performed by the agent self
        actions = torch.tensor([action[self] for action in actions])[self_alive]
        
        # set logits for unavailable actions to very low value
        for idx in self_alive
            logits[idx,:] = -np.infty
        
        # compute the target value and the resulting advantage value
        target = rewards + gamma * next_vals * (1.0 - dones)
        advantage = target.detach() - values.squeeze()
        advantage = advantage[self_alive_idx]        
        
        # compute the log probabilities for the chosen actions
        log_prob = F.log_softmax(logits, dim=-1)
        log_prob_act = log_prob[self_alive_idx, actions]
        log_prob_act_val = advantage.detach() * log_prob_act
        
        # compute the entropy over the resulting policy distribution
        probs = F.softmax(logits, dim=1)
        entropy = -(probs * log_prob).sum(dim=1)
        loss_entropy = entropy.mean()
        
        # compute policy and value losses and the global loss function
        loss_pol = -log_prob_act_val.mean()
        loss_val = advantage.pow(2).mean()
        loss = loss_pol + loss_val - beta * loss_entropy
        
        # feed loss backward through computational grad and take update step
        self.model.optimizer.zero_grad()
        loss.backward()
        self.model.optimizer.step()

def train():
    # create the agents and put them into teams
    team_blue = [IACAgent() for _ in range(n_friends)]
    team_red  = [Agent()    for _ in range(n_enemies)]
    training_agents = team_blue
    agents = team_blue + team_red
    
    # create the environment
    env = Environment(agents, args)

    # generate the neural network model and assign it to the training agents  
    models = generate_model(input_shape=n_inputs, n_actions=n_actions)
    for agent in training_agents:
        agent.set_models(models)

    # start the train sequence
    for step_idx in range(int(n_steps):
        batch = []
        for _ in range(n_episodes_per_step):
            # generate an episode and add it to the batch
            episode = generate_episode(env)
            batch.extend(episode)

        for agent in training_agents:
            agent.update(batch)
            if step_idx % 50 == 0: # sync target network
                agent.sync_models()
\end{minted}

\chapter{QMix}
\label{app:qmix}
\begin{minted}{python}

class QMIXAgent(Agent):

    def act(self, obs, test_mode=False):
        "Select an action based on an observation"
        unavail_actions = self.env.get_unavailable_actions()[self]
        avail_actions = [action for action in self.actions
                        if action not in unavail_actions]

        qvals, self.hidden_state = self.model(obs, self.hidden_state)
        # remove unavailable actions
        for action in unavail_actions:
            qvals[0][action.id] = -np.infty
        action_idx = qvals.max(1)[1].item() # pick position of maximum
    
        if test_mode: # when in test_mode, always return 'best' action
            return self.actions[action_idx]
        
        # epsilon-greedy action selection
        eps = self.scheduler()
        if random.random() < eps:
            return random.choice(avail_actions)
        else:
            return self.actions[action_idx]

class MultiAgentController:
    """ Controls the different learning agents
        and their update process
    """
    def __init__(self, env, agents, models):
        # ...
        
        # create mixing network
        self.mixer        = QMixer()
        self.target_mixer = copy.deepcopy(self.mixer)
        self.sync_networks()
        self.parameters = list(self.model.parameters())
        self.parameters += list(self.mixer.parameters())
        self.optimizer = torch.optim.Adam(self.parameters, lr=lr)

    def update(self, batch):
        "Perform one update step according to the QMix algorithm"
        batch_size = len(batch)
        states, next_states, observations, next_obs, hidden, next_hidden, actions,\
            rewards, dones, unavail = self._build_inputs(batch)
        
        # reshape observations & hidden states to push them through network models
        observations = observations.reshape(batch_size * len(self.agents), -1)
        next_obs = next_obs.reshape(batch_size * len(self.agents), -1)
        hidden = hidden.reshape(batch_size * len(self.agents), -1)
        next_hidden = next_hidden.reshape(batch_size * len(self.agents), -1)
        
        # compute & reshape estimated Q-values
        current_q_vals,   _ = self.model(observations, hidden)
        predicted_q_vals, _ = self.target(next_obs, next_hidden)
        current_q_vals = current_q_vals.reshape(batch_size, len(self.agents), -1)
        predicted_q_vals = predicted_q_vals.reshape(batch_size, len(self.agents), -1)

        # gather q-vals corresponding to the actions taken
        current_q_vals_actions = current_q_vals[range(batch_size * len(self.agents))
                                                , actions.reshape(-1)]

        predicted_q_vals[unavail==1] = -1e10 # set unavailable actions to low value
        
        # pick maximum Q-value for Bellman update
        predicted_q_vals_max = predicted_q_vals.max(2)[0]
        
        # use mixer to mix all Q-vals
        current_q_tot   = self.mixer(current_q_vals_actions, states)
        predicted_q_tot = self.target_mixer(predicted_q_vals_max, next_states)
        
        # Bellman update
        target = rewards + gamma * (1. - dones) * predicted_q_tot
        td_error = current_q_tot - target
        loss = (td_error ** 2).mean()

        # Perform update
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()
    
    def sync_networks(self):
        self.target.load_state_dict(self.model.state_dict())
        self.target_mixer.load_state_dict(self.mixer.state_dict())
        
def train():
    team_blue = [QMIXAgent() for idx in range(n_friends)] 
    team_red  = [Agent()     for idx in range(n_enemies)] 
    training_agents = team_blue
    agents = team_blue + team_red
    env = Environment(agents, args)

    models = generate_models(n_inputs, n_actions)
    for agent in training_agents:
        agent.set_model(models)

    buffer = ReplayBuffer(size=buffer_size)
    mac = MultiAgentController(env, training_agents, models)
    for step_idx in range(n_steps):
        episode = generate_episode(env)
        buffer.insert_list(episode)
        if len(buffer) < batch_size:
            continue
        batch = buffer.sample(batch_size)
        
        loss = mac.update(batch)
        
        # synchronize target networks 
        if step_idx % args.sync_interval == 0:
            mac.sync_networks()
\end{minted}

\end{appendices}


